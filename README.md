

# RuATD (Russian Artificial Text Detection)

[English](https://github.com/dialogue-evaluation/RuATD/blob/main/en_README.md)



## Мотивация

Современные модели генерации текстов показывают впечатляющие результаты: они могут сочинить стихотворение,  изменить стиль текстов и даже написать осмысленное эссе на свободную тематику. Однако такие модели могут быть использованы в злонамеренных целях, например, для генерации фейковых новостей, отзывов на продукты и политического контента. Так, возникает новая задача: научиться отличать тексты, написанные человеком, от текстов, сгенерированных нейросетевыми языковыми моделями. 

## Полезные ссылки

- Бинарная постановка: [kaggle](https://www.kaggle.com/c/ruatd-2022-bi/)
- Мультиклассовая постановка: [kaggle](https://www.kaggle.com/c/ruatd-2022-multi-task/)
- Группа в телеграм: [tlg](https://t.me/ruatd)

## Постановка задачи

Соревнование RuATD (Russian Artificial Text Detection) посвящено задаче автоматического распознавания сгенерированных текстов и предлагает участникам рассмотреть две постановки:

1. Определить, был ли текст сгенерирован автоматически или написан человеком;
2. Определить, какая именно модель была использована для генерации данного текста.

С формальной точки зрения, первая задача является задачей бинарной классификации, а вторая – мультиклассовой классификации. Обучающие и тестовые данные размечены автоматически. Тексты, написанные человеком, собраны из открытых источников. Различные нейросетевые языковые модели  – машинного перевода, парафразирования, суммаризации, упрощения и безусловной генерации текстов – использованы для генерации текстов. 

## Разметка и фрагмент данных

Схема бинарной разметки содержит следующие обозначения:

- H – текст написан человеком
- M – текст сгенерирован автоматически

Схема мультиклассовой разметки содержит следующие обозначения:

- OPUS-MT – текст сгенерирован моделью машинного перевода OPUS
- ruGPT3-Large – текст сгенерирован моделью ruGPT3-Large
- и так далее

Файлы [sample_submit_binary](https://github.com/dialogue-evaluation/RuATD/blob/main/sample_submit_binary.csv) и [sample_submit_multiple](https://github.com/dialogue-evaluation/RuATD/blob/main/sample_submit_multiple.csv) представляют формат данных для отправки на платформу соревнования. 

Пример обучающих данных представлен в таблице ниже. 

| H | M-MT (FR→RU) |
| --- | --- |
| Эх, у меня может быть и нет денег, но у меня всё ещё есть гордость. | Может, у меня нет денег, но у меня всегда есть гордость. |
| Меня покусали комары. | Меня похитили муски. |
| Я не могу чувствовать себя в гостинице как дома. | Я не могу чувствовать себя дома в отеле. |
| Эта книга показалась мне интересной. | Я нашёл эту интересную книгу. |
| Я был полон решимости помочь ему, даже рискуя собственной жизнью. | Я был готов помочь ему в опасности своей жизни. |
| Моя квартира находится меньше чем в пяти минутах пешком от станции. | Моя квартира находится на расстоянии менее пяти минут от станции. |

## Оценка решений

Для оценки решений в соревновании будет использована стандартная метрика оценки качества классификации — доля правильных ответов модели (accuracy).

## Базовые решения

Организаторы предоставят два базовых решения задачи:

- **tf-idf** + логистическая регрессия
- дообучение модели ruBERT

Код базовых решений будет доступен в репозитории соревнования. 

## Правила участия

1. Участникам разрешается использовать **любые дополнительные материалы** и любые предобученные модели, за исключением непосредственной разметки тестового множества и поиска в интернете.
2. В соревновании могут участвовать и **команды**, и **отдельные** участники. 
3. По результатам соревнования участники будут приглашены подать статьи в сборник конференции Диалог 2022. 
4. После завершения соревнования будет организован **дополнительный раунд** оценки решений участников: мы попросим опубликовать все решения в открытом доступе и проверить, что участники не пользовались поиском в интернете и других источниках. 

## Ориентировочные сроки проведения соревнования

- Конец декабря 2021 - начало января 2022 – публикация обучающих данных
- 17 января 2022 – открытие платформ тестирования
- 25 февраля 2022, 9 утра (Мск) – закрытие тестирования
- 1-2 марта 2022 - официальное подведение итогов
- 15 марта 2022 – завершаем прием статей

## Организационный комитет

Екатерина Артемова (НИУ ВШЭ, Huawei Noah’s Ark Lab)

Анастасия  Валеева (МФТИ)

Константин Николаев (НИУ ВШЭ)

Владислав Михайлов (SberDevices)

Марат Саидов (НИУ ВШЭ)

Иван Смуров (ABBYY, МФТИ)

Елена Тутубалина (Sber)

Алена Феногенова (SberDevices)

Даниил Чернявский (Skolkovo Institute of Science and Technology)

Татьяна Шаврина (AIRI, SberDevices)

Татьяна Шамардина (ABBYY)
