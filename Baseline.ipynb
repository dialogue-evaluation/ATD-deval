{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b045da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296400e",
   "metadata": {},
   "source": [
    "# Baseline notebook\n",
    "\n",
    "В данном ноутбуке показан бейзлайн для предсказания искуственности текста.\n",
    "\n",
    "Бейзлайн работает одинаково для обоих треков - binary и multiclass. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008cd891",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "Finetune BERT. Используется предобученный DeepPavlov ruBERT, доступный на HuggingFace hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dabe2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "model_name = 'DeepPavlov/rubert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfcb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/' # директория с данными \n",
    "train = pd.read_csv(data_dir + 'train.csv')\n",
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "val = pd.read_csv(data_dir + 'val.csv')\n",
    "\n",
    "le = LabelEncoder() # закодируем лейблы \n",
    "le.fit(train['Class'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd37a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(input_data):\n",
    "    texts, labels = zip(*input_data)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding='longest', max_length=256, truncation=True)\n",
    "    inputs['Class'] = labels\n",
    "    return inputs\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, sort=False, le=None):\n",
    "        super().__init__()\n",
    "        self.texts = data['Text'].values\n",
    "        if 'Class' in data.columns: # если есть разметка\n",
    "            assert not data['Class'].isnull().any(), \"Some labels are null\"\n",
    "            if le is not None:\n",
    "                self.labels = le.transform(data['Class'])\n",
    "            else:\n",
    "                self.labels = data['Class'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if hasattr(self, 'labels'):\n",
    "            return self.texts[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.texts[idx], []\n",
    "\n",
    "class Metric: # metric class for storing metrics (accuracy, loss)\n",
    "    def __init__(self):\n",
    "        self.storage = defaultdict(list)\n",
    "    \n",
    "    def store(self, **kwargs):\n",
    "        for key in kwargs:\n",
    "            self.storage[key].append(kwargs[key])\n",
    "            \n",
    "    def reset(self):\n",
    "        self.storage.clear()\n",
    "        \n",
    "    def log(self):\n",
    "        for key in self.storage:\n",
    "            self.storage[key] = np.mean(self.storage[key])\n",
    "        return self.storage.items()\n",
    "        \n",
    "class BertClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name, lr=1e-5, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.metric = Metric()\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.bert.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bert(**x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('Class')\n",
    "        logits = self.bert(**batch).logits\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        predictions = logits.argmax(axis=1)\n",
    "        accuracy = torch.mean((predictions == labels).double())\n",
    "        self.metric.store(loss=loss.item(), accuracy=accuracy.item())\n",
    "        if batch_idx % 100: # every 100 batches - log metrics (mean of last 100 batches)\n",
    "            for k,v in self.metric.log():\n",
    "                self.log(f'train/{k}', v)\n",
    "            self.metric.reset()\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('Class')\n",
    "        logits = self.bert(**batch).logits\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self.log('val/loss', loss)\n",
    "        predictions = logits.argmax(axis=1)\n",
    "        self.log('val/accuracy', torch.mean((predictions == labels).double()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TextDataset(train, le=le)\n",
    "val = TextDataset(val, le=le)\n",
    "test = TextDataset(test, le=le)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff02430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertClassifier(model_name, num_labels=len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54de51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "version = f\"{model_name}_binary\"\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=os.getcwd(), name='lightning_logs', version=version)\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger, \n",
    "    gpus=[1],\n",
    "    max_epochs=3, \n",
    "    num_sanity_val_steps=1\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.bert.state_dict(), \"./deeppavlov_bert_trained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317b2ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy_and_pred(model, loader): # используйте эту функцию для получения accuracy и предсказаний\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    labels = None\n",
    "    accs = 0\n",
    "    ns = 0\n",
    "    for batch in tqdm(loader):\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(model.device)\n",
    "        labels = batch.pop('Class')\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch).logits.argmax(axis=1)\n",
    "        if labels.size()[1] > 0:\n",
    "            accs += torch.sum((pred == labels).double())\n",
    "        preds.append(pred.cpu().numpy())\n",
    "        ns += len(pred)\n",
    "    return accs/ns, np.concatenate(preds)\n",
    "\n",
    "acc, preds = get_accuracy_and_pred(model, test_loader)\n",
    "np.save('./preds/test_preds_bert.npy', le.inverse_transform(preds))\n",
    "print(f\"Test accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156d2ae",
   "metadata": {},
   "source": [
    "### TD-IDF baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ed43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bf287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(data, le):\n",
    "    data['Class'] = le.transform(data['Class'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f58681",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./stopwords_ru.txt', encoding='utf-8') as fp:\n",
    "    stopwords = [s.strip() for s in fp.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./ruatd-multi/train.csv')\n",
    "test = pd.read_csv('./ruatd-multi/test.csv')\n",
    "val = pd.read_csv('./ruatd-multi/val.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(train['Class'].values)\n",
    "\n",
    "for d in [train, val]:\n",
    "    d = encode_labels(d, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords, ngram_range=(1, 3), max_features=50000)), \n",
    "    ('svd', TruncatedSVD(n_components=5000)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(C=0.01, n_jobs=40))\n",
    "], verbose=True)\n",
    "\n",
    "X_train = np.concatenate((train['Text'].values, val['Text'].values))\n",
    "y_train = np.concatenate((train['Class'].values, val['Class'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f657987",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08de7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('preds/test_preds_tfidf.npy', le.inverse_transform(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1944b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
